{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Inference of Pre-trained Models on Jetson Nano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why does VEX AI Robotics Competition (VAIRC) utilize pre-trained models for inference on the Jetson nano?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pre-trained model refers to a model that has been trained on a large-scale dataset and is then used as a starting point for models targeting specific tasks.\\\n",
    "The core idea of pre-trained models is to enable the model to learn rich feature representations and knowledge through training on vast amounts of general data, and then fine-tune it based on these foundations to adapt to specific tasks.\\\n",
    "Advantages:\\\n",
    "Reduced Training Time: Pre-trained models have already been trained on vast amounts of data and possess good initial weights, therefore significantly reducing the training time for specific tasks.\\\n",
    "Improved Performance: As pre-trained models have learned rich feature representations and knowledge, they typically perform well on specific tasks.\n",
    "Data Efficiency: For tasks with limited labeled data, pre-trained models can utilize the available data more effectively through transfer learning.\\\n",
    "Applications:\\\n",
    "Pre-trained models are widely used in areas such as natural language processing (NLP) and computer vision. Here are some common applications:\\\n",
    "Natural Language Processing: Pre-trained models like BERT, GPT-3, RoBERTa, etc., have demonstrated excellent performance in tasks such as text classification, sentiment analysis, machine translation, and text generation.\\\n",
    "Computer Vision: Pre-trained models like VGG, ResNet, Inception, etc., are widely applied in tasks like image classification, object detection, and image segmentation.\\\n",
    "Workflow:\\\n",
    "Pre-training: The model is trained on a large-scale unsupervised or weakly-supervised dataset to learn general feature representations. For example, training the BERT model on a large corpus of text data.\\\n",
    "Fine-tuning: The model is fine-tuned on labeled data for a specific task to adapt to that particular task. For instance, fine-tuning a pre-trained BERT model using data from a classification task.\\\n",
    "For example:\\\n",
    "ResNet (Residual Networks):\\\n",
    "Pre-training: The ResNet model is trained in a supervised manner on the ImageNet dataset, learning rich image feature representations.\\\n",
    "Fine-tuning: The pre-trained ResNet model is fine-tuned in a supervised manner for a specific image classification task.\\\n",
    "GPT (Generative Pre-trained Transformer) also incorporates the technique of pre-trained models.\\\n",
    "Pre-trained models are machine learning models that are trained on large-scale datasets and then applied to specific tasks. They offer significant advantages in terms of improving training efficiency, reducing computational resource consumption, and enhancing model performance. \\\n",
    "Through pre-training and fine-tuning, pre-trained models are able to achieve good performance on various tasks.\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using external pre-trained models on the Jetson Nano edge computing device offers numerous advantages, particularly for efficient AI computing in resource-constrained environments. Here are some key benefits:\\\n",
    "Saving Training Time and Computational Resources:\\\n",
    "Pre-trained models have already undergone training on large-scale datasets, thus they can be directly used for inference tasks. This significantly reduces the need for training time and computational resources, making them suitable for low-power devices like the Jetson Nano.\\\n",
    "Efficient Real-time Processing:\\\n",
    "The Jetson Nano's GPU acceleration capabilities enable it to efficiently run deep learning models. By using pre-trained models, real-time processing can be achieved on edge devices for tasks such as video stream analysis, image classification, and object detection. Utilizing NVIDIA TensorRT for high-speed inference avoids the need to install development frameworks like PyTorch or TensorFlow, saving on computational resources.\\\n",
    "Reduced Data Requirement:\n",
    "Pre-trained models have been trained on vast amounts of data and are able to generalize well. This means that for fine-tuning on specific tasks, only a small amount of specialized data is needed to achieve good performance, making them suitable for data-constrained application scenarios.\n",
    "Flexible Model Deployment:\\\n",
    "Pre-trained models are often modular, allowing for pruning and optimization based on specific applications. This flexibility enables easier deployment on the Jetson Nano, allowing the adjustment of model size and complexity based on actual needs.\n",
    "Enhanced Intelligence at the Edge:\\\n",
    "Using pre-trained models enables complex AI functionalities to be implemented on edge devices, such as intelligent surveillance, autonomous driving assistance, and robot navigation. This enhances the intelligence and autonomy of the devices.\n",
    "Reduced Development Cost and Time:\\\n",
    "Pre-trained models provide high-quality baseline models that developers can fine-tune and optimize, significantly reducing development cost and time while improving development efficiency.\\\n",
    "Many pre-trained models (such as ResNet, MobileNet, YOLO, etc.) enjoy widespread community support and a wealth of resources (including open-source code, tutorials, documentation, etc.), making it easier for developers to get started and troubleshoot issues quickly.\\\n",
    "Optimized Performance and Energy Efficiency:\\\n",
    "Pre-trained models are carefully designed and optimized to achieve high performance with low energy consumption on low-power devices like the Jetson Nano. This extends the battery life of the devices.\\\n",
    "In summary, using external pre-trained models on the Jetson Nano edge computing device not only improves development efficiency and deployment flexibility but also enables efficient real-time AI computing in resource-constrained environments, bringing powerful intelligent functionalities to edge devices.\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As a reference material for VAIRC programmers, this technical topic discusses how to effectively run pre-trained models on the Jetson nano. It begins with training a working model on a regular personal computer (Windows, macOS) and then transferring it to the Jetson nano environment for operation.\\\n",
    "For readers unfamiliar with neural network programming and model training, this is a brief introduction and learning opportunity for these knowledge areas. The main goal of this topic is to ensure readers understand and successfully achieve the implementation of a trained model running on the Jetson nano.\\\n",
    "Fine-tuning an external pre-trained model for specific tasks involves more knowledge and programming skills in model development, which will be discussed in detail in the next topic.\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Regarding the utilization of external devices for training, ordinary personal computers, regardless of whether they are running Windows or macOS, can be used to train networks that will run on the Jetson nano, regardless of whether they have a GPU or not.\\\n",
    "Considering that it would greatly assist the reader's understanding if they can run the demonstrated programs concurrently with reading this document, this article is written in the form of a .ipynb file. Under the condition that the necessary runtime environment is configured, the reader can read and run the programs simultaneously, enhancing the efficiency and interest of learning.\\\n",
    "Previously mentioned, to run on a commonly used PC, appropriate configuration is required, including the installation of a program editor, Python compiler, and PyTorch runtime environment. \\\n",
    "The installation of the runtime environment is detailed in the \"<windows安装anaconda和pytorch环境.md>\" and \"<macOS安装anaconda和pytorch环境.md>\" files. Please refer to these guides for the development environment installation, or search online for alternative installation methods.\\\n",
    "Alternatively, TensorFlow can also be chosen as the development framework. While the two frameworks have different syntax rules and styles, they both operate within the Python environment, and the algorithmic essence is largely the same. They are both popular and mainstream development platforms today. Some possibly biased evaluations in the industry suggest that PyTorch is more intuitive and clear, making it friendly for beginners, which is why PyTorch is selected here.\\\n",
    "The programs introduced in this article need to be run in two separate environments (the training of the neural network model uses an external PC, while the model inference program runs on the Jetson nano platform). Red font will be used in the text to provide hints about the runtime environment, and we hope you will pay attention during the execution of the programs.\\\n",
    "Now, let's begin the narrative on the training program for handwritten digit recognition neural networks, using a combined approach of programming and explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This article will use the training and inference of the classic handwritten digit recognition neural network (MNIST) from the history of neural network development as an example to introduce the methods of training deep learning neural networks. It will provide a brief overview for beginners without prior experience in neural network model training on the PyTorch development platform, explaining the process of running the training programs. However, it will not delve deeply into the Python and PyTorch programming rules and syntax explanations. If needed, please refer to other resources for further learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " </p>\n",
    " <span style=\"color: red;\">Note: The following code segments are to be run on external devices (Windows, macOS computers) unless otherwise indicated.\n",
    " <span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Training a Handwritten Digit Recognition Model(MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------训练开始\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('-------------训练开始')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Importing necessary library functions:\\\n",
    "torch is the library function of PyTorch, and the PyTorch development framework supports the Python programming environment by calling the torch library functions.\\\n",
    "torchvision is a sub-library of PyTorch, used for processing image and video data.\\\n",
    "matplotlib is a 2D plotting library for Python, which provides a wide range of plotting tools that can be used in Python scripts, Jupyter notebooks, web application servers, and four graphical user interface toolkits. matplotlib is an open-source project based on Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Check if a GPU can be used. If the computer has GPU hardware installed and the GPU drivers are correctly installed, the program will automatically run in a GPU environment. Otherwise, the program will run in a CPU environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Hyperparameters are parameters that are set before the learning process begins, and their choice directly affects the performance and behavior of machine learning models. They need to be set by users or algorithm developers before the training process.\\\n",
    "Setting hyperparameters:\n",
    "\n",
    "batch_size: In machine learning and deep learning, batch_size refers to the number of training samples used in one iteration during the training process. It is an important hyperparameter that has a direct impact on the training efficiency and effectiveness of the model.\n",
    "\n",
    "learning_rate: In machine learning and deep learning, the learning rate is a crucial hyperparameter that controls the speed of updating the model's parameters during training. The value of the learning rate determines the magnitude of the change in parameters during each iteration. Specifically, the learning rate determines the size of the step taken in updating the weights during the gradient descent process.\n",
    "\n",
    "num_epochs: In machine learning and deep learning, num_epochs (or the number of epochs) refers to the number of times the entire training dataset is traversed through for training. Understanding num_epochs is crucial for optimizing the training process and achieving good model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在机器学习和图像处理中，需要对图像进行预处理，它使用了 PyTorch 的 transforms 模块。这里通过 transforms.Compose 创建了一个变换序列，这些变换会按顺序应用到图像数据上。具体来说，它包含了两个步骤：<br>\n",
    "transforms.ToTensor()：这个变换将 PIL 图像或者 NumPy 数组转换成 PyTorch 张量。它会改变数据格式，从 H x W x C（高度 x 宽度 x 通道）变为 C x H x W（通道 x 高度 x 宽度），并且将数据类型从 uint8（取值范围是 0-255）转换为 float32（取值范围是 0.0-1.0）。这一步是数据准备的重要部分，因为 PyTorch 模型通常预期输入是张量形式。<br>\n",
    "transforms.Normalize((0.5,), (0.5,))：这个变换进行数据标准化，使得输入数据的均值和标准差达到指定值。这里的参数 (0.5,) 为均值，(0.5,) 为标准差。由于传入的是单通道的灰度图像（只有一个通道），所以这里只有一个值。这种标准化有助于模型训练，因为它确保了不同的输入特征在相同的尺度上，有利于梯度下降算法更快更稳定地收敛。<br>\n",
    "这种预处理流程是机器学习中常用的技术，特别是在处理图像时，可以帮助提高模型的性能和训练速度。<br>  \n",
    "\n",
    "In machine learning and image processing, it is necessary to preprocess images, which often involves using the transforms module from PyTorch. A sequence of transformations is created using transforms.Compose, and these transformations are applied to the image data in order. Specifically, it consists of two steps:\n",
    "\n",
    "transforms.ToTensor(): This transformation converts a PIL image or a NumPy array into a PyTorch tensor. It changes the data format from H x W x C (height x width x channels) to C x H x W (channels x height x width) and converts the data type from uint8 (values ranging from 0-255) to float32 (values ranging from 0.0-1.0). This step is an important part of data preparation as PyTorch models typically expect inputs in the form of tensors.\n",
    "\n",
    "transforms.Normalize((0.5,), (0.5,)): This transformation normalizes the data to have a specified mean and standard deviation. Here, the parameters (0.5,) represent the mean, and (0.5,) represent the standard deviation. Since grayscale images with a single channel are being processed, there is only one value for each. This normalization helps with model training as it ensures that different input features are on the same scale, facilitating faster and more stable convergence of the gradient descent algorithm.\n",
    "\n",
    "This preprocessing pipeline is a commonly used technique in machine learning, especially when dealing with images, and can help improve the model's performance and training speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " Before executing the next program, create two directories named \"data\" and \"model\" in the current directory. These directories will be used to store the downloaded training data and the trained network models.\n",
    "</p> <span style=\"color: yellow;\">Note: If the \"data\" and \"model\" directories do not exist, the program execution will encounter an error. <span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./手写数字图像.png\" alt=\"Example Plot\" style=\"width: 50%; height: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The MNIST (Modified National Institute of Standards and Technology) dataset is a classic handwritten digit dataset widely used in research and education for machine learning and deep learning, often serving as an introductory training and testing resource for computer vision and machine learning. It contains 28x28 pixel grayscale images representing digits from 0 to 9. The MNIST dataset is divided into a training set and a test set, with the following specifics:\\\n",
    "Training set: 60,000 images\\\n",
    "Test set: 10,000 images\\\n",
    "Each image is a single-channel grayscale image with pixel values ranging from 0 to 255. Typically, when using the dataset, the pixel values are normalized to be between 0 and 1 or -1 and 1, as shown in the figure above.\\\n",
    "Here's a detailed explanation of the code that uses the PyTorch framework to load and prepare the MNIST dataset:\\\n",
    "Loading the Dataset\n",
    "Creating the Dataset Object:\\\n",
    "The datasets.MNIST() function from the torchvision library is called to load the MNIST dataset.\\\n",
    "root='./data' specifies the local path where the data will be stored. If the data is not found in the specified path, it will be downloaded from the internet.\\\n",
    "train=True and train=False indicate whether to load the training set or the test set.\\\n",
    "transform=transform applies predefined transformations (such as converting to tensors and normalizing) that are applied to each image during data loading.\\\n",
    "download=True allows automatic data downloading from the internet if the data is not available locally.\\\n",
    "Creating the DataLoader\\\n",
    "Configuring the DataLoader:\\\n",
    "torch.utils.data.DataLoader() is used to create an iterable data loader, which enables batch-wise data loading during model training and testing.\n",
    "dataset=train_dataset and dataset=test_dataset specify the source of the data.\\\n",
    "batch_size=batch_size is a hyperparameter that specifies the number of images loaded in each batch. This value affects the memory consumption and speed of model training.\\\n",
    "shuffle=True and shuffle=False determine whether the training dataset should be shuffled at the beginning of each training epoch. Shuffling the training data can help reduce model overfitting, while the test data is typically not shuffled.\\\n",
    "By setting these parameters, this code effectively prepares the data streams for training and testing, ensuring that the data is appropriately processed and batched before entering the model. Such a data processing pipeline is one of the crucial steps in implementing effective deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class ComplexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)  # Input channels, Output channels, Kernel size\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Kernel size, Stride\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 600)\n",
    "        self.fc2 = nn.Linear(600, 120)\n",
    "        self.fc3 = nn.Linear(120, 10)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor for the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This code defines a deep learning model called ComplexNet, built on the PyTorch framework. It is a convolutional neural network (CNN), commonly used for processing image data.\\\n",
    "Class Definition and Initialization\\\n",
    "ComplexNet inherits from nn.Module, which is the base class for all PyTorch neural networks.\n",
    "In the __init__ method, the various layers of the network are defined. These layers include convolutional layers, pooling layers, fully connected layers (linear layers), and a Dropout layer.\\\n",
    "(I) Layer Composition\\\n",
    "Convolutional Layers\\\n",
    "self.conv1: The first convolutional layer uses a 3x3 convolution kernel with 1 input channel and 32 output channels. Padding of 1 is used to maintain the spatial size of the input and output.\\\n",
    "self.conv2: The second convolutional layer also uses a 3x3 convolution kernel, but with 32 input channels and 64 output channels, maintaining the same padding strategy.\\\n",
    "Pooling Layer\\\n",
    "self.pool: A max-pooling layer uses a 2x2 window to downsample the feature map, with a stride of 2. This helps reduce the spatial dimensions and number of parameters of the data, thereby controlling overfitting.\\\n",
    "Fully Connected Layers\\\n",
    "self.fc1: The first fully connected layer takes the flattened features from the convolutional layers, which are of size 6477, and outputs 600.\\\n",
    "self.fc2: The second fully connected layer further reduces the features from 600 dimensions to 120.\\\n",
    "self.fc3: The final fully connected layer reduces the features from 120 dimensions to 10, which typically represents the number of classes in a classification task.\\\n",
    "Dropout Layer\\\n",
    "self.dropout: The Dropout layer drops neurons with a probability of 0.25, a common technique to prevent overfitting.\\\n",
    "(II) Forward Propagation Method (forward)\\\n",
    "The forward method defines how data flows through these layers:\\\n",
    "Data first passes through the self.conv1 convolutional layer, followed by the ReLU activation function, and then through the pooling layer.\\\n",
    "After passing through the second convolutional layer self.conv2 and the ReLU activation, it goes through the pooling layer again.\\\n",
    "The pooled data needs to be flattened to serve as input for the fully connected layers.\\\n",
    "The flattened data passes through three fully connected layers (the first one with ReLU activation and Dropout, the second one with only ReLU activation), and finally, the final result is output through self.fc3.\\\n",
    "Such a network architecture is suitable for image classification tasks, capable of automatically learning useful features from raw pixels to make classification decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model建立\n"
     ]
    }
   ],
   "source": [
    "#实例化网络并移动到设备上：\n",
    "#创建了ComplexNet的一个实例，并将其移动到了之前检测到的设备上（GPU或CPU）。\n",
    "model = ComplexNet().to(device)\n",
    "print('model建立')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "An instance of ComplexNet has been created and moved to the previously detected device (GPU or CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This code defines two key components for training a neural network, which are commonly set up in the PyTorch framework.\\\n",
    "Loss Function:\\\n",
    "criterion = nn.CrossEntropyLoss()\\\n",
    "nn.CrossEntropyLoss() is the cross-entropy loss function in PyTorch used for classification problems. It combines nn.LogSoftmax() and nn.NLLLoss() (negative log-likelihood loss) into a single class. It is well-suited for multiclass classification problems where the classes are mutually exclusive. The loss function expects the model's output to be the raw, unnormalized scores (also known as logits), and the labels should be the class indices for each sample.\\\n",
    "Optimizer:\\\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optim.SGD is the implementation of Stochastic Gradient Descent, one of the most basic optimization algorithms.\\\n",
    "model.parameters() provides references to all the trainable parameters in the model, which will be updated by the optimizer. \\\n",
    "lr=learning_rate sets the learning rate, which is a crucial hyperparameter controlling the step size of the parameter updates. The size of the learning rate directly affects the speed and quality of the model parameter updates during training.\\\n",
    "These two components work together, enabling the model to adjust its parameters based on the loss computed by the loss function using each batch of data during the training loop. The goal is to improve the model's performance and gradually reduce the loss value. In a practical training loop, the data from each batch is used to calculate the loss, and then the optimizer updates the model weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "这段代码涉及使用 Python 的 matplotlib 库来实时更新和显示训练过程中的损失和准确率图表。这是数据科学和机器学习中常用的做法，可以帮助开发者直观地了解模型的训练进展和性能。下面是对代码中每部分的具体解释：\n",
    "\n",
    "实时交互模式\n",
    "plt.ion()：\n",
    "plt.ion() 开启 matplotlib 的交互模式。在这个模式下，plt.show() 命令不会阻塞代码的执行，允许图表在显示的同时继续运行更新。\n",
    "这对于实时更新图表（如在训练过程中动态显示损失和准确率变化）非常有用。\n",
    "初始化图表\n",
    "损失图表 (fig_loss, ax_loss)：\n",
    "\n",
    "fig_loss, ax_loss = plt.subplots() 创建一个新的图表和坐标轴用于绘制损失值。\n",
    "fig_loss 是图表对象，可以用来对图表进行全局设置，如图表大小、标题等。\n",
    "ax_loss 是对应的坐标轴对象，用来具体绘制和设置图表中的元素，如线条样式、坐标轴标签、图例等。\n",
    "准确率图表 (fig_acc, ax_acc)：\n",
    "\n",
    "fig_acc, ax_acc = plt.subplots() 创建另一个图表和坐标轴用于绘制准确率。\n",
    "这允许损失和准确率分别在不同的窗口中实时更新，使得两者的变化趋势可以分别观察，避免在单一图表中信息过载。\n",
    "使用这种方式，你可以在模型训练的每个周期（epoch）或每个批次（batch）结束后更新这些图表，展示最新的训练统计信息。这对于调试模型和调整训练参数非常有帮助，因为你可以即时看到修改参数对模型性能的影响  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练和评估\n",
    "train_losses = []\n",
    "train_accuracies = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Two lists, train_losses and train_accuracies, are initialized to store the loss and accuracy values for each iteration (typically each batch or epoch) during the model training process. This allows tracking and visualizing the performance changes during training, which is crucial for understanding and adjusting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3946, Accuracy:0.5956\n",
      "Epoch 2, Loss: 0.2565, Accuracy:0.9230\n",
      "Epoch 3, Loss: 0.1503, Accuracy:0.9548\n",
      "Epoch 4, Loss: 0.1098, Accuracy:0.9667\n",
      "Epoch 5, Loss: 0.0881, Accuracy:0.9734\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.ion()\n",
    "# 初始化两个图表\n",
    "fig_loss, ax_loss = plt.subplots()\n",
    "fig_acc, ax_acc = plt.subplots()\n",
    "with open('./model/training_log.txt', 'w') as f:\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        #------------------------------------------\n",
    "            _,predicted = torch.max(outputs.data,1)\n",
    "            total_predictions += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "        #------------------------------------------\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = correct_predictions/total_predictions\n",
    "            # 保存checkpoint\n",
    "        checkpoint = {\n",
    "            #'name': 'VAIC-MNISTComplexNet',\n",
    "            #'maker': 'Lichengtong ',\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': running_loss / len(train_loader)\n",
    "        }\n",
    "        torch.save(checkpoint, f'./model/mnist_complex_net_checkpoint_epoch{epoch + 1}.pth')\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "        print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Accuracy:{epoch_accuracy:.4f}')\n",
    "        f.write(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\\n')\n",
    "\n",
    "        #更新图表并保存训练结果：\n",
    "        #在每个epoch结束后，更新图表以显示最新的损失和准确率，并在训练结束后保存图表为图片文件。\n",
    "        ax_loss.clear()\n",
    "        ax_loss.plot(train_losses, 'r-', label='Loss')\n",
    "        ax_loss.set_title('Training Loss')\n",
    "        ax_loss.set_xlabel('Epoch')\n",
    "        ax_loss.set_ylabel('Loss')\n",
    "        fig_loss.canvas.draw()\n",
    "        ax_acc.clear()\n",
    "        ax_acc.plot(train_accuracies, 'g-', label='Accuracy')\n",
    "        ax_acc.set_title('Training Accuracy')\n",
    "        ax_acc.set_xlabel('Epoch')\n",
    "        ax_acc.set_ylabel('Accuracy')\n",
    "        fig_acc.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This code snippet represents a training loop that incorporates model training and logging the training progress to a file.\\\n",
    "Let's break down each part and its functionality:\\\n",
    "File Logging\\\n",
    "Using with open('./model/training_log.txt', 'w') as f opens a file for writing the logs from the training process. This approach ensures that the file is properly closed once the code block is executed.\\\n",
    "Training Process\\\n",
    "num_epochs = 5 sets the number of training epochs to 5, which is a hyperparameter that can be adjusted before the training starts.\\\n",
    "for epoch in range(num_epochs): initiates a loop, where each iteration represents one complete training epoch.\n",
    "Within each epoch:\\\n",
    "running_loss = 0.0 initializes the cumulative loss.\\\n",
    "correct_predictions = 0 and total_predictions = 0 initialize counters for calculating accuracy.\\\n",
    "For each batch:\\\n",
    "for i, data in enumerate(train_loader, 0): iterates through the data batches from train_loader.\\\n",
    "inputs, labels = data[0].to(device), data[1].to(device) transfers the inputs and labels to the computational device (such as a GPU).\\\n",
    "optimizer.zero_grad() clears previous gradient information.\\\n",
    "outputs = model(inputs) obtains the model's output for the current batch.\\\n",
    "loss = criterion(outputs, labels) calculates the loss.\\\n",
    "loss.backward() performs backpropagation.\\\n",
    "optimizer.step() updates the model's parameters based on the gradients.\\\n",
    "running_loss += loss.item() accumulates the loss to compute the average loss.\\\n",
    "_, predicted = torch.max(outputs.data, 1) retrieves the predicted results.\\\n",
    "total_predictions += labels.size(0) updates the total number of predictions.\\\n",
    "correct_predictions += (predicted == labels).sum().item() updates the number of correct predictions.\\\n",
    "Calculating and Logging Results:\\\n",
    "epoch_loss = running_loss / len(train_loader) calculates the average loss for the current epoch.\\\n",
    "epoch_accuracy = correct_predictions / total_predictions calculates the accuracy for the current epoch.\\\n",
    "torch.save(checkpoint, f'./model/mnist_complex_net_checkpoint_epoch{epoch + 1}.pth') Here, the torch.save() function is used to save the checkpoint dictionary to disk. The file path and name are dynamically generated using f-string (a feature introduced in Python 3.6+), which includes the epoch number. This way, the checkpoint for each epoch is saved as a separate file, with the epoch number included in the filename, making it easy to find and load a specific checkpoint in the future.\\\n",
    "train_losses.append(epoch_loss) and train_accuracies.append(epoch_accuracy) record the loss and accuracy, respectively.\\\n",
    "The code provides a comprehensive demonstration of the basic steps involved in training a neural network using PyTorch, including data processing, model training, loss computation, backpropagation, parameter updates, result calculation, and logging. By recording the logs to a file, you can easily track the detailed performance changes during training, which is very helpful for debugging and improving the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using the Python's matplotlib library within a training loop to update and display real-time charts of the training loss and accuracy is a common practice in data science and machine learning. It allows developers to visually understand the model's training progress and performance. Here is a detailed explanation of each part:\\\n",
    "Interactive Mode\\\n",
    "plt.ion():\\\n",
    "plt.ion() enables matplotlib's interactive mode. In this mode, the plt.show() command will not block the execution of the code, allowing the chart to be displayed while continuing to run and update.\\\n",
    "This is useful for updating charts dynamically, such as displaying the changing loss and accuracy during training.\\\n",
    "Chart Initialization\\\n",
    "Loss Chart (fig_loss, ax_loss):\\\n",
    "fig_loss, ax_loss = plt.subplots() creates a new figure and axes for plotting the loss values.\\\n",
    "fig_loss is the figure object that allows global settings on the figure, such as its size and title.\\\n",
    "ax_loss is the corresponding axes object that is used to specifically plot and set elements in the figure, such as line styles, axis labels, and legends.\\\n",
    "Accuracy Chart (fig_acc, ax_acc):\\\n",
    "fig_acc, ax_acc = plt.subplots() creates another figure and axes for plotting the accuracy.\\\n",
    "This allows the loss and accuracy to be updated separately in different windows, enabling the trends of both to be observed independently, avoiding information overload in a single chart.\\\n",
    "Using this approach, you can update these charts after each training epoch or batch, showing the latest training statistics. This is helpful for debugging the model and adjusting training parameters as you can immediately see the impact of parameter changes on the model's performance.\\\n",
    "Dynamically Updating the Charts for Training Loss and Accuracy After Each Epoch\\\n",
    "After each training epoch, you can update the charts to reflect the latest training loss and accuracy. This provides a visual representation of the model's learning progress and performance changes.\\\n",
    "Saving the Charts as Image Files After the Entire Training Process\\\n",
    "It is also a useful practice to save these charts as image files after the entire training process is complete. This allows you to keep a record of the model's training progress and compare it with future runs or experiments.\\\n",
    "The process of visualizing the model's learning progress and performance changes through charts is invaluable as it provides insights into how the model is behaving and how it can be further improved.\\\n",
    "Updating Charts\\\n",
    "At the end of each training epoch, the charts are updated to reflect the latest training loss and accuracy. The specific steps include:\\\n",
    "Clearing Old Chart Content: Use ax_loss.clear() and ax_acc.clear() to remove previous chart content. This is done to remove old data points before plotting new ones.\\\n",
    "Plotting New Data:\\\n",
    "For the loss chart: Use ax_loss.plot(train_losses, 'r-', label='Loss') to plot the loss data, where 'r-' specifies the line color and style (red solid line).\\\n",
    "For the accuracy chart: Use ax_acc.plot(train_accuracies, 'g-', label='Accuracy') to plot the accuracy data, where 'g-' specifies the line color and style (green solid line).\\\n",
    "Setting Chart Titles and Axis Labels:\\\n",
    "Set the title, X-axis, and Y-axis labels for the loss chart.\\\n",
    "Do the same for the accuracy chart.\\\n",
    "Redrawing the Charts: Use fig_loss.canvas.draw() and fig_acc.canvas.draw() to trigger a redraw of the charts, making the updated data visible on the charts.\\\n",
    "Saving Charts as Image Files\\\n",
    "After the entire training process is complete, you can save these charts as image files using fig_loss.savefig('training_loss.png') and fig_acc.savefig('training_accuracy.png'). \\\n",
    "This allows you to document the training results and facilitates subsequent analysis and reporting.\\\n",
    "Note that the code lines to save the image files should be executed after all training epochs are completed, typically outside the training loop.\\\n",
    "By doing this, you can ensure that the results of each training session are effectively recorded and visualized, which is an essential tool for evaluating and comparing model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------show2\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "plt.ioff()\n",
    "plt.show()\n",
    "#保存图\n",
    "fig_loss.savefig('fig_loss.png')\n",
    "fig_acc.savefig('fig_acc.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Turning Off Interactive Mode with plt.ioff():\\\n",
    "This turns off the interactive mode of matplotlib. The interactive mode (enabled with plt.ion()) allows charts to be updated in real-time during training without blocking the program's execution. Once training is complete and there is no need for real-time updates, you can turn off the interactive mode to prepare for final display or saving static images.\\\n",
    "Displaying Charts:\\\n",
    "plt.show(): This command is used to display all active matplotlib figure windows. It is typically used in non-interactive mode to present the graphical interface after the program has finished executing. When the interactive mode is off, using this command ensures that users have an opportunity to view the final charts.\\\n",
    "Saving Charts as Images:\\\n",
    "fig_loss.savefig('fig_loss.png') and fig_acc.savefig('fig_acc.png'): These two lines of code save the training loss chart and training accuracy chart as PNG image files, respectively. This is a very useful feature because it allows you to keep a record of the charts for reporting or further analysis.\\\n",
    "'fig_loss.png' and 'fig_acc.png' are the file save paths and names. You can modify these parameters according to your needs to save in different locations or use different file names.\\\n",
    "By doing this, you can not only view the performance changes during training on the screen, but also have a permanent record, which is very helpful for subsequent evaluation and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------end\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('----------训练结束')】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### II. Verification of the Recognition Functionality of the Handwritten Digit Recognition Model\n",
    "Here, readers are encouraged to test the recognition performance of the trained model. They can appropriately modify the network structure of the model (by increasing or decreasing the number of layers, neurons, etc.) and test the changes in recognition performance after training. AI tools such as GPT-4 or Copilot can also be utilized to assist in modifying the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model建立\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 设置随机种子以确保结果可复现\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# 定义模型\n",
    "class ComplexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)  # Input channels, Output channels, Kernel size\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Kernel size, Stride\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 600)\n",
    "        self.fc2 = nn.Linear(600, 120)\n",
    "        self.fc3 = nn.Linear(120, 10)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor for the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 检查是否有可用的GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 实例化模型\n",
    "model = ComplexNet().to(device)\n",
    "print('model建立')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this code is identical to the part where the model's network structure is defined during training, it does not need to be executed again if you are validating the model immediately after training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 加载训练好的模型参数，只加载 model_state_dict 部分\n",
    "checkpoint = torch.load('./model/mnist_complex_net_checkpoint_epoch50.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()  # 设置为评估模式\n",
    "\n",
    "# 定义数据预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# 加载测试数据\n",
    "test_size = 100\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=test_size, shuffle=False)\n",
    "# 获取一个批次的数据\n",
    "data, target = next(iter(test_loader))\n",
    "\n",
    "# 将数据输入模型进行推理\n",
    "data = data.to(device)\n",
    "output = model(data)\n",
    "pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "# 计算推理错误的个数并记录错误的行列位置\n",
    "incorrect = (pred.cpu() != target.view_as(pred)).sum().item()\n",
    "incorrect_indices = (pred.cpu() != target.view_as(pred)).nonzero(as_tuple=True)[0]\n",
    "\n",
    "print(f'推理错误的个数: {incorrect} / 100')\n",
    "\n",
    "# 设置显示样本的网格\n",
    "fig, axs = plt.subplots(10, 10, figsize=(15, 15))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# 显示每个样本及其预测结果\n",
    "for i in range(test_size):\n",
    "    axs[i].imshow(data[i].cpu().numpy().squeeze(), cmap='gray')\n",
    "    title = f'Pred: {pred[i].item()}\\nTrue: {target[i].item()}'\n",
    "    if i in incorrect_indices:\n",
    "        row, col = divmod(i, 10)\n",
    "        title += f'\\nError: ({row}, {col})'\n",
    "    axs[i].set_title(title)\n",
    "    axs[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 显示推理错误的图像\n",
    "if incorrect > 0:\n",
    "    rows = (incorrect // 10) + (1 if incorrect % 10 != 0 else 0)\n",
    "    fig_error, axs_error = plt.subplots(rows, 10, figsize=(15, 3 * rows))\n",
    "    axs_error = axs_error.flatten()\n",
    "\n",
    "    for idx, error_idx in enumerate(incorrect_indices):\n",
    "        axs_error[idx].imshow(data[error_idx].cpu().numpy().squeeze(), cmap='gray')\n",
    "        row, col = divmod(error_idx.item(), 10)\n",
    "        axs_error[idx].set_title(f'Pred: {pred[error_idx].item()}\\nTrue: {target[error_idx].item()}\\n({row}, {col})')\n",
    "        axs_error[idx].axis('off')\n",
    "\n",
    "    for ax in axs_error[incorrect:]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This code is primarily used to load a trained model and perform inference (i.e., predictions) on the MNIST dataset. It then displays a portion of the prediction results, highlighting those images that were incorrectly predicted. Here is a detailed explanation of the code:\\\n",
    "Loading the trained model parameters:\\ \n",
    "Defining data preprocessing:\\\n",
    "Uses transforms.Compose to define a series of data preprocessing steps, including converting the data to PyTorch tensors (ToTensor) and normalizing the data (Normalize).\\\n",
    "Loading the test data:\\\n",
    "Uses datasets.MNIST (or a similar function/class) to load the MNIST test dataset.\\\n",
    "Creates a data loader (DataLoader) with a specified batch_size (e.g., batch_size=100), which means that 100 samples will be loaded for testing at a time. \\\n",
    "Note that for the entire MNIST test set, which contains 10,000 images, multiple batches would typically be used, but for simplicity, the code might assume a small batch_size or process the entire set as a single batch.\\\n",
    "Gets the first batch of data using next(iter(test_loader)). In the case where the batch_size is set to the size of the entire test set, this will retrieve the only batch.\\\n",
    "Model inference:\\\n",
    "Moves the loaded data to the appropriate device (e.g., CPU or GPU) using the device-specific functions or methods (e.g., data.to(device)).\n",
    "Performs inference on the loaded data using the loaded model. This typically involves passing the data through the model to obtain \n",
    "\n",
    "Using a model for inference on input data involves several steps:\\\n",
    "Finding the Predicted Class for Each Sample:\\\n",
    "Apply the argmax method to find the index of the maximum predicted probability for each sample, which represents the predicted class.\n",
    "Calculating the Number of Incorrect Predictions:\\\n",
    "Compare the predictions with the true labels to calculate the number of incorrect predictions.\\\n",
    "Identifying Indices of Incorrect Predictions:\\\n",
    "Use the nonzero method to find the indices of the samples where the predictions were incorrect.\\\n",
    "Displaying Samples and Their Predictions:\\\n",
    "Use plt.subplots to create a 10x10 grid for displaying samples.\\\n",
    "For each sample, use imshow to display the image and set_title to set the title, which includes the predicted class and the true label.\n",
    "If the prediction for a sample is incorrect, include an additional indication of the error in the title.\\\n",
    "Displaying Images with Incorrect Predictions:\\\n",
    "If there are samples with incorrect predictions, create a new grid to display these incorrect samples.\\\n",
    "Similarly, use imshow to display the images and set_title to set the title, which includes the predicted class, the true label, and an indication of the error.\\\n",
    "In summary, this code provides a complete workflow from loading a model to performing inference on a test set and visualizing the prediction results, both correct and incorrect. This helps users intuitively understand the model's performance and identify potential error patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Converting a Network from .pth Format to .onnx Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These import statements include libraries for defining and training neural networks (such as torch and torchvision), as well as libraries for data processing and visualization (such as numpy and matplotlib)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ComplexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)  # Input channels, Output channels, Kernel size\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Kernel size, Stride\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 600)\n",
    "        self.fc2 = nn.Linear(600, 120)\n",
    "        self.fc3 = nn.Linear(120, 10)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor for the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义了一个卷积神经网络类 ComplexNet。这个网络包含两个卷积层、一个最大池化层和三个全连接层。每个卷积层后面都跟着一个ReLU激活函数和一个池化层。全连接层中间插入了一个Dropout层，以减少过拟合。<br>\n",
    "此网络定义与训练时网络结构相同。<br>\n",
    "A convolutional neural network class named ComplexNet has been defined. This network consists of two convolutional layers, a max pooling layer, and three fully connected layers. Each convolutional layer is followed by a ReLU activation function and a pooling layer. A Dropout layer is inserted between the fully connected layers to reduce overfitting.\\\n",
    "The network definition is identical to the network structure used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查是否有可用的GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if a GPU is available, and if so, use the GPU; otherwise, use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model建立---\n"
     ]
    }
   ],
   "source": [
    "# 实例化模型\n",
    "model = ComplexNet().to(device)\n",
    "print('model建立---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the ComplexNet model, ensuring that it is consistent with the model structure used during training, and move it to the device (CPU or GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint keys: dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss'])\n",
      "Model weights:\n",
      "conv1.weight: torch.Size([32, 1, 3, 3])\n",
      "conv2.weight: torch.Size([64, 32, 3, 3])\n",
      "fc1.weight: torch.Size([600, 3136])\n",
      "fc2.weight: torch.Size([120, 600])\n",
      "fc3.weight: torch.Size([10, 120])\n"
     ]
    }
   ],
   "source": [
    "# 加载训练好的模型参数，只加载 model_state_dict 部分\n",
    "checkpoint_path = './model/mnist_complex_net_checkpoint_epoch5.pth'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# 打印 checkpoint 的所有键\n",
    "print(\"Checkpoint keys:\", checkpoint.keys())\n",
    "\n",
    "# 打印模型的权重（仅打印部分权重示例）\n",
    "print(\"Model weights:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        print(f\"{name}: {param.data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained model parameters from the .pth file and populate the model with them.\\\n",
    "Print all the keys in the checkpoint file and some weight information to verify that the loading was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been exported to ONNX format at ./model/mnist_complex_net_li.onnx.\n"
     ]
    }
   ],
   "source": [
    "# 导出为 ONNX 格式\n",
    "dummy_input = torch.randn(1, 1, 28, 28).to(device)  # MNIST 输入尺寸\n",
    "onnx_path = './model/mnist_complex_net_li.onnx'\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    ")\n",
    "\n",
    "print(f\"Model has been exported to ONNX format at {onnx_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use torch.onnx.export to export the model to ONNX format, specifying the names of the input and output tensors as well as the dynamic axes to support different batch sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Performing Inference Using TensorRT on a Jetson Nano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "</p> <span style=\"color: red;\">Note: Please run the following program segment on a Jetson Nano edge computing device.\\\n",
    "<span> \n",
    "</p> <span style=\"color: yellow;\">Before executing the next program segment, create two new directories named \"data\" and \"model\" in the current directory. Copy the trained and converted .onnx file from an external device to the \"model\" directory. <br>\n",
    "Optionally, you can also copy the training data from an external device to the \"data\" directory to avoid having to re-download it during the Jetson's runtime.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorRT 日志记录器\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "# ONNX 模型路径\n",
    "onnx_model_path = './model/mnist_complex_net.onnx'\n",
    "engine_path = './model/mnist_complex_net.trt'\n",
    "\n",
    "# 将 ONNX 模型转换为 TensorRT 引擎\n",
    "def build_engine(onnx_file_path, engine_file_path):\n",
    "    with trt.Builder(TRT_LOGGER) as builder, builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "        builder.max_workspace_size = 1 << 30  # 1GB\n",
    "        builder.max_batch_size = 1\n",
    "        if not parser.parse_from_file(onnx_file_path):\n",
    "            for error in range(parser.num_errors):\n",
    "                print(parser.get_error(error))\n",
    "            return None\n",
    "        with builder.build_cuda_engine(network) as engine:\n",
    "            with open(engine_file_path, \"wb\") as f:\n",
    "                f.write(engine.serialize())\n",
    "            return engine\n",
    "\n",
    "# 构建引擎\n",
    "engine = build_engine(onnx_model_path, engine_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码定义了一个函数 build_engine，用于从 ONNX 模型文件构建 TensorRT 引擎。TensorRT 是 NVIDIA 提供的一个用于高性能深度学习推理（inference）的库。以下是代码的逐步解释：\n",
    "函数定义:\n",
    "def build_engine(onnx_file_path, engine_file_path):\n",
    "定义了一个函数 build_engine，它接受两个参数：\n",
    "onnx_file_path：ONNX 模型的文件路径。\n",
    "engine_file_path：TensorRT 引擎将要保存的文件路径。\n",
    "TensorRT 初始化:\n",
    "with trt.Builder(TRT_LOGGER) as builder, ...:\n",
    "这里使用 with 语句来确保 TensorRT 的资源（如 builder 和 network）在使用完毕后被正确释放。trt.Builder 是 TensorRT 的一个类，用于构建引擎。TRT_LOGGER 是一个日志记录器，用于记录 TensorRT 生成的日志。\n",
    "同时，代码创建了一个 network 对象，该对象用于定义网络的拓扑结构。这里使用了 EXPLICIT_BATCH 标志，意味着网络将显式地处理批处理大小。\n",
    "接下来，创建了一个 OnnxParser 对象，用于从 ONNX 模型文件中解析网络结构。\n",
    "1. 设置 TensorRT 参数:\n",
    "builder.max_workspace_size = 1 << 30  # 1GB  \n",
    "builder.max_batch_size = 1\n",
    "\n",
    "Here is the translation of the provided code explanation into English, along with a slight modification to include the actual code snippet for clarity:\\\n",
    "Function Definition:\\\n",
    "def build_engine(onnx_file_path, engine_file_path):\\\n",
    "This defines a function called build_engine that takes two parameters:\\\n",
    "onnx_file_path: The file path of the ONNX model.\\\n",
    "engine_file_path: The file path where the TensorRT engine will be saved.\\\n",
    "TensorRT Initialization:\\\n",
    "with trt.Builder(TRT_LOGGER) as builder, builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, trt.OnnxParser(builder, network, TRT_LOGGER) as parser:\\\n",
    "Here, the with statement is used to ensure that TensorRT resources (such as the builder and network) are properly released after use. trt.Builder is a class from TensorRT used to build engines. TRT_LOGGER is a logger for recording TensorRT-generated logs.\n",
    "Simultaneously, a network object is created, which defines the topology of the network. The EXPLICIT_BATCH flag is set, meaning that the network will explicitly handle batch sizes.\n",
    "An OnnxParser object is also created, which is used to parse the network structure from the ONNX model file.\n",
    "Setting TensorRT Parameters:\n",
    "builder.max_workspace_size = 1 << 30  # 1GB  \n",
    "builder.max_batch_size = 1\n",
    "These lines set the TensorRT parameters:\n",
    "builder.max_workspace_size is set to 1GB (1 << 30, which is equivalent to 2 raised to the power of 30, or 1,073,741,824 bytes). This is the maximum GPU memory that TensorRT is allowed to use for workspace during engine building and execution.\n",
    "builder.max_batch_size is set to 1, indicating that the engine will only support a batch size of 1.\n",
    "The max_workspace_size sets the maximum amount of GPU memory that TensorRT can use when executing the network (here it's 1GB).\\\n",
    "The max_batch_size sets the maximum batch size for the engine (here it's \\\n",
    "1).Parsing the ONNX model:\\\n",
    "if not parser.parse_from_file(onnx_file_path):\\\n",
    "    ...\\\n",
    "Using the parse_from_file method of the OnnxParser to parse the model from the specified ONNX file path. If parsing fails, it prints out the error from the parser.\\\n",
    "2）Building the TensorRT engine:\\\n",
    "with builder.build_cuda_engine(network) as engine:  \\\n",
    "    ...\\\n",
    "Using the build_cuda_engine method of the builder to construct a CUDA engine from the previously defined network.\\\n",
    "3）Saving and returning the engine:\\\n",
    "with open(engine_file_path, \"wb\") as f:  \\\n",
    "    f.write(engine.serialize())  \\\n",
    "return engine\\\n",
    "Serializing the constructed engine into byte data and saving it to the specified file path. Additionally, the engine object is returned for further use if needed.\\\n",
    "Function call:\\\n",
    "engine = build_engine(onnx_model_path, engine_path)\\\n",
    "By providing the path to the ONNX model and the path to save the engine, the build_engine function is called to construct a TensorRT engine.\\\n",
    "In summary, the purpose of this code is to build a TensorRT engine from an ONNX model file, and save the engine to a specified file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集路径（假设是MNIST数据集）\n",
    "mnist_image_path = './data/MNIST/raw/t10k-images-idx3-ubyte'\n",
    "mnist_label_path = './data/MNIST/raw/t10k-labels-idx1-ubyte'\n",
    "\n",
    "# 读取 MNIST 数据集\n",
    "def load_mnist_images(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        images = np.fromfile(f, dtype=np.uint8).reshape(num, 1, rows, cols)\n",
    "    return images\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num = struct.unpack(\">II\", f.read(8))\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "# 加载图像和标签\n",
    "images = load_mnist_images(mnist_image_path)\n",
    "labels = load_mnist_labels(mnist_label_path)\n",
    "\n",
    "# 选择若干图像进行推理（例如前10个）\n",
    "num_images = 10\n",
    "selected_images = images[:num_images]\n",
    "selected_labels = labels[:num_images]\n",
    "\n",
    "# 预处理图像\n",
    "def preprocess_image(image):\n",
    "    image = image.astype(np.float32)\n",
    "    image /= 255.0  # 归一化\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preparation of inference data, including reading and normalization, is the same as that of test data in the validation phase on external computing devices. There is no need to elaborate on this here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 加载 TensorRT 引擎\n",
    "def load_engine(engine_file_path):\n",
    "    with open(engine_file_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "        return runtime.deserialize_cuda_engine(f.read())\n",
    "\n",
    "engine = load_engine(engine_path)\n",
    "context = engine.create_execution_context()\n",
    "\n",
    "# 准备输入和输出缓冲区\n",
    "input_shape = (1, 1, 28, 28)\n",
    "output_shape = (1, 10)  # 假设是10个类别的分类\n",
    "input_data = np.zeros(input_shape, dtype=np.float32)\n",
    "output_data = np.empty(output_shape, dtype=np.float32)\n",
    "\n",
    "d_input = cuda.mem_alloc(input_data.nbytes)\n",
    "d_output = cuda.mem_alloc(output_data.nbytes)\n",
    "bindings = [int(d_input), int(d_output)]\n",
    "\n",
    "# 推理并显示结果\n",
    "for i in range(num_images):\n",
    "    image = preprocess_image(selected_images[i])\n",
    "    cuda.memcpy_htod(d_input, image)\n",
    "\n",
    "    # 进行推理\n",
    "    context.execute_v2(bindings)\n",
    "\n",
    "    cuda.memcpy_dtoh(output_data, d_output)\n",
    "    predicted_label = np.argmax(output_data)\n",
    "    true_label = selected_labels[i]\n",
    "\n",
    "    # 显示图像和推理结果\n",
    "    plt.imshow(image[0], cmap='gray')\n",
    "    plt.title(f'Predicted: {predicted_label}, True: {true_label}')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码的主要目的是使用TensorRT来加载一个预先序列化的TensorRT引擎，并使用该引擎对MNIST数据集中的图像进行推理。下面是对该代码的逐行解释：\n",
    "加载 TensorRT 引擎：\n",
    "\n",
    "def load_engine(engine_file_path):  \n",
    "    with open(engine_file_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:  \n",
    "        return runtime.deserialize_cuda_engine(f.read())\n",
    "这定义了一个函数load_engine，它打开一个包含TensorRT引擎的文件，并使用TensorRT运行时（trt.Runtime）反序列化该文件以加载引擎。这里假设trt是TensorRT库的别名，并且TRT_LOGGER是一个已定义的日志记录器。\n",
    "加载引擎并创建执行上下文：\n",
    "\n",
    "engine = load_engine(engine_path)  \n",
    "context = engine.create_execution_context()\n",
    "这两行代码首先调用load_engine函数来加载TensorRT引擎，然后为该引擎创建一个执行上下文。\n",
    "准备输入和输出缓冲区：\n",
    "The main purpose of this code is to use TensorRT to load a pre-serialized TensorRT engine and use this engine to perform inference on images from the MNIST dataset. Below is a line-by-line explanation of the code:\\\n",
    "\n",
    "Loading the TensorRT Engine:\\\n",
    "def load_engine(engine_file_path):  \\\n",
    "    with open(engine_file_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:  \\\n",
    "        return runtime.deserialize_cuda_engine(f.read())\\\n",
    "This defines a function load_engine that opens a file containing the TensorRT engine and uses the TensorRT runtime (trt.Runtime) to deserialize the file to load the engine. Here, it is assumed that trt is an alias for the TensorRT library and TRT_LOGGER is a pre-defined logger.\\\n",
    "\n",
    "Loading the Engine and Creating an Execution Context:\\\n",
    "engine = load_engine(engine_path)  \\\n",
    "context = engine.create_execution_context()\\\n",
    "These two lines of code first call the load_engine function to load the TensorRT engine and then create an execution context for the engine.\\\n",
    "\n",
    "Preparing Input and Output Buffers:\\\n",
    "\n",
    "input_shape = (1, 1, 28, 28)  \n",
    "output_shape = (1, 10)  # 假设是10个类别的分类  \n",
    "input_data = np.zeros(input_shape, dtype=np.float32)  \n",
    "output_data = np.empty(output_shape, dtype=np.float32)\n",
    "这里定义了输入和输出的形状，并初始化了对应的NumPy数组。input_data用于存储输入图像数据，而output_data用于存储推理结果。\n",
    "分配CUDA设备内存并设置绑定：\n",
    "d_input = cuda.mem_alloc(input_data.nbytes)  \n",
    "d_output = cuda.mem_alloc(output_data.nbytes)  \n",
    "bindings = [int(d_input), int(d_output)]\n",
    "这里使用CUDA库（假设cuda是CUDA库的别名）为输入和输出数据分配设备内存。然后，创建一个绑定列表，该列表将主机内存（input_data和output_data）与设备内存（d_input和d_output）关联起来。\n",
    "推理并显示结果：\n",
    "\n",
    "input_shape = (1, 1, 28, 28)  \\\n",
    "output_shape = (1, 10)  # Assume it is a classification with 10 categories  \\\n",
    "input_data = np.zeros(input_shape, dtype=np.float32)  \\\n",
    "output_data = np.empty(output_shape, dtype=np.float32)\\\n",
    "Here, the shapes of the input and output are defined, and the corresponding NumPy arrays are initialized. input_data is used to store the input image data, while output_data is used to store the inference results.\\\n",
    "\n",
    "Allocating CUDA Device Memory and Setting Bindings:\\\n",
    "\n",
    "d_input = cuda.mem_alloc(input_data.nbytes)  \\\n",
    "d_output = cuda.mem_alloc(output_data.nbytes)  \\\n",
    "bindings = [int(d_input), int(d_output)]\\\n",
    "Here, the CUDA library (assuming cuda is an alias for the CUDA library) is used to allocate device memory for the input and output data. \\Then, a binding list is created to associate host memory (input_data and output_data) with device memory (d_input and d_output).\\\n",
    "\n",
    "Inference and Displaying Results:\\\n",
    "\n",
    "for i in range(num_images):  \n",
    "    image = preprocess_image(selected_images[i])  \n",
    "    cuda.memcpy_htod(d_input, image)  \n",
    "  \n",
    "    # 进行推理  \n",
    "    context.execute_v2(bindings)  \n",
    "  \n",
    "    cuda.memcpy_dtoh(output_data, d_output)  \n",
    "    predicted_label = np.argmax(output_data)  \n",
    "    true_label = selected_labels[i]  \n",
    "  \n",
    "    # 显示图像和推理结果  \n",
    "    plt.imshow(image[0], cmap='gray')  \n",
    "    plt.title(f'Predicted: {predicted_label}, True: {true_label}')  \n",
    "    plt.axis('off')  \n",
    "    plt.show()\n",
    "\n",
    "This loop iterates over the selected number of images (num_images). \\\n",
    "For each image, it first performs preprocessing, then copies the preprocessed image from host memory to device memory. \\\n",
    "Next, it uses the TensorRT execution context to perform inference on the image.\\\n",
    "After inference is completed, it copies the results from device memory back to host memory and finds the category with the highest predicted probability as the predicted label. Finally, it uses the Matplotlib library (assuming plt is an alias for Matplotlib) to display the original image and the inference result.\\\n",
    "Note: In a real application, it may be necessary to handle various errors and exceptions, as well as optimize performance (e.g., by processing images in batches or using streams to overlap data transfer and computation). \\\n",
    "Additionally, it is essential to ensure that the CUDA and TensorRT libraries are correctly installed and that all imports and dependencies in the code are satisfied.\\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
